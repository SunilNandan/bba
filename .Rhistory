doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (56196)
term.frequency <- as.integer(term.table)
# MCMC and model tuning parameters:
K <- 10
G <- 3000
alpha <- 0.02
eta <- 0.02
# Fit the model:
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
library(quanteda)
library(stm)
library(tm)
library(NLP)
library(openNLP)
library(ggplot2)
library(ggdendro)
library(cluster)
library(fpc)
require(quanteda)
setwd("C:/Users/sunil/Google Drive/Sem 3/Business analytics/Assignments/Assignment 5")
speechdf<- read.csv("trumpVICTORYspeech.csv",
header=TRUE, stringsAsFactors=FALSE)
speechcorpus<- corpus(speechdf$Speech,
docnames=NULL,
docvar=NULL)
speechcorpus<- toLower(speechcorpus, keepAcronyms = FALSE)
cleancorpus <- tokenize(speechcorpus,
removeNumbers=TRUE,
removePunct = TRUE,
removeSeparators=TRUE,
removeTwitter=FALSE,
verbose=TRUE)
dfm<- dfm(cleancorpus,
toLower = TRUE,
ignoredFeatures =stopwords("english"),
verbose=TRUE,
stem=TRUE)
?stopwords
topfeatures(dfm, 50)     # displays 50 features
dfm.tm<-convert(dfm, to="tm")
dfm.tm
dtmss <- removeSparseTerms(dfm.tm, 0.15)
dtmss
d.dfm <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d.dfm, method="average")
hcd <- as.dendrogram(fit)
hcd
require(cluster)
k<-5
plot(hcd, ylab = "Distance", horiz = FALSE,
main = "Five Cluster Dendrogram",
edgePar = list(col = 2:3, lwd = 2:2))
rect.hclust(fit, k=k, border=1:5) # draw dendogram with red borders around the 5 clusters
ggdendrogram(fit, rotate = TRUE, size = 4, theme_dendro = FALSE,  color = "blue") +
xlab("Features") +
ggtitle("Cluster Dendrogram") #ideal for summarising a document
require(fpc)
d <- dist(t(dtmss), method="euclidian")
kfit <- kmeans(d, 5)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
require(fpc)
d <- dist(t(dtmss), method="euclidian")
kfit <- kmeans(d, 5)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
View(speechdf)
topfeatures(dfm, 50)     # displays 50 features
news_2015<-speechdf$Speech
stop_words <- stopwords("SMART")
stop_words <- c(stop_words, "said", "the", "also", "say", "just", "like","for",
"us", "can", "may", "now", "year", "according", "mr")
stop_words <- tolower(stop_words)
news_2015 <- gsub("'", "", news_2015) # remove apostrophes
news_2015 <- gsub("[[:punct:]]", " ", news_2015)  # replace punctuation with space
news_2015 <- gsub("[[:cntrl:]]", " ", news_2015)  # replace control characters with space
news_2015 <- gsub("^[[:space:]]+", "", news_2015) # remove whitespace at beginning of documents
news_2015 <- gsub("[[:space:]]+$", "", news_2015) # remove whitespace at end of documents
news_2015 <- gsub("[^a-zA-Z -]", " ", news_2015) # allows only letters
news_2015 <- tolower(news_2015)  # force to lowercase
news_2015 <- news_2015[news_2015 != ""]
doc.list <- strsplit(news_2015, "[[:space:]]+")
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
term.table <- term.table[names(term.table) != ""]
vocab <- names(term.table)
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
D <- length(documents)  # number of documents (1)
W <- length(vocab)  # number of terms in the vocab (1741)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (56196)
term.frequency <- as.integer(term.table)
K <- 10
G <- 3000
alpha <- 0.02
eta <- 0.02
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
t2 <- Sys.time()
## display runtime
t2 - t1
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
news_for_LDA <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
library(LDAvis)
library(servr)
# create the JSON object to feed the visualization:
json <- createJSON(phi = news_for_LDA$phi,
theta = news_for_LDA$theta,
doc.length = news_for_LDA$doc.length,
vocab = news_for_LDA$vocab,
term.frequency = news_for_LDA$term.frequency)
serVis(json, out.dir = 'vis', open.browser = TRUE)
library(quanteda)
library(stm)
library(tm)
library(NLP)
library(openNLP)
setwd("C:/Users/sunil/Google Drive/Sem 3/Business analytics/Assignments/Assignment 5")
precorpus<- read.csv("Corpus1.csv",
header=TRUE, stringsAsFactors=FALSE)
names(precorpus)
companycorpus<- corpus(precorpus$Mission.statement.and.core.values,
docnames=precorpus$Company.name,
docvar=NULL)
names(companycorpus)
summary(companycorpus)
companycorpus<- toLower(companycorpus, keepAcronyms = FALSE)
cleancorpus <- tokenize(companycorpus,
removeNumbers=TRUE,
removePunct = TRUE,
removeSeparators=TRUE,
removeTwitter=FALSE,
verbose=TRUE)
head(cleancorpus)
str(cleancorpus)
swlist = c("a, about, above, across, after, again, against, all, almost, alone, along, already, also, although, always, am, among, an, and, another, any, anybody, anyone, anything, anywhere, are, area, areas, aren't, around, as, ask, asked, asking, asks, at, away, b, back, backed, backing, backs, be, became, because, become, becomes, been, before, began, behind, being, beings, below, best, better, between, big, both, but, by, c, came, can, cannot, can't, case, cases, certain, certainly, clear, clearly, come, could, couldn't, d, did, didn't, differ, different, differently, do, does, doesn't, doing, done, don't, down, downed, downing, downs, during, e, each, early, either, end, ended, ending, ends, enough, even, evenly, ever, every, everybody, everyone, everything, everywhere, f, face, faces, fact, facts, far, felt, few, find, finds, first, for, four, from, full, fully, further, furthered, furthering, furthers, g, gave, general, generally, get, gets, give, given, gives, go, going, good, goods, got, great, greater, greatest, group, grouped, grouping, groups, h, had, hadn't, has, hasn't, have, haven't, having, he, he'd, he'll, her, here, here's, hers, herself, he's, high, higher, highest, him, himself, his, how, however, how's, i, i'd, if, i'll, i'm, important, in, interest, interested, interesting, interests, into, is, isn't, it, its, it's, itself, i've, j, just, k, keep, keeps, kind, knew, know, known, knows, l, large, largely, last, later, latest, least, less, let, lets, let's, like, likely, long, longer, longest, m, made, make, making, man, many, may, me, member, members, men, might, more, most, mostly, mr, mrs, much, must, mustn't, my, myself, n, necessary, need, needed, needing, needs, never, new, newer, newest, next, no, nobody, non, noone, nor, not, nothing, now, nowhere, number, numbers, o, of, off, often, old, older, oldest, on, once, one, only, open, opened, opening, opens, or, order, ordered, ordering, orders, other, others, ought, our, ours, ourselves, out, over, own, p, part, parted, parting, parts, per, perhaps, place, places, point, pointed, pointing, points, possible, present, presented, presenting, presents, problem, problems, put, puts, q, quite, r, rather, really, right, room, rooms, s, said, same, saw, say, says, second, seconds, see, seem, seemed, seeming, seems, sees, several, shall, shan't, she, she'd, she'll, she's, should, shouldn't, show, showed, showing, shows, side, sides, since, small, smaller, smallest, so, some, somebody, someone, something, somewhere, state, states, still, such, sure, t, take, taken, than, that, that's, the, their, theirs, them, themselves, then, there, therefore, there's, these, they, they'd, they'll, they're, they've, thing, things, think, thinks, this, those, though, thought, thoughts, three, through, thus, to, today, together, too, took, toward, turn, turned, turning, turns, two, u, under, until, up, upon, us, use, used, uses, v, very, w, want, wanted, wanting, wants, was, wasn't, way, ways, we, we'd, well, we'll, wells, went, were, we're, weren't, we've, what, what's, when, when's, where, where's, whether, which, while, who, whole, whom, who's, whose, why, why's, will, with, within, without, won't, work, worked, working, works, would, wouldn't, x, y, year, years, yes, yet, you, you'd, you'll, young, younger, youngest, your, you're, yours, yourself, yourselves, you've, z") # stop words list
dfm.stem<- dfm(cleancorpus, toLower = TRUE,
ignoredFeatures = c(swlist, stopwords("english")),
verbose=TRUE,
stem=TRUE)
topfeatures.stem<-topfeatures(dfm.stem, n=50)
topfeatures.stem
cleancorpus <- tokenize(companycorpus,
removeNumbers=TRUE,
removePunct = TRUE,
removeSeparators=TRUE,
removeTwitter=FALSE )
dfm.stem<- dfm(cleancorpus, toLower = TRUE,
ignoredFeatures = c(swlist, stopwords("english")),
verbose=TRUE,
stem=TRUE)
topfeatures.stem<-topfeatures(dfm.stem, n=50)
topfeatures.stem
library(wordcloud)
set.seed(142)   #keeps cloud' shape fixed
dark2 <- brewer.pal(7, "Set1")
freq<-topfeatures(dfm.stem, n=500)
help("wordcloud")
wordcloud(names(freq),
freq, max.words=200,
scale=c(3, .1),
colors=brewer.pal(8, "Set1"))
dfm.tm<-convert(dfm.stem, to="tm")
names(dfm.tm)
findAssocs(dfm.tm,
c("data", "busi", "compani","custom","oracl","world","enterprise","make","big","solut","analyt"),
corlimit=0.7)
library(stm)
help("textProcessor")
temp<-textProcessor(documents=precorpus$Mission.statement.and.core.values, metadata = precorpus)
names(temp)
meta<-temp$meta
vocab<-temp$vocab
docs<-temp$documents
out <- prepDocuments(docs, vocab, meta)
docs<-out$documents
vocab<-out$vocab
meta <-out$meta
prevfit <-stm(docs , vocab ,
K=5,
verbose=TRUE,
data=meta,
max.em.its=5)
topics <-labelTopics(prevfit , topics=c(1:5))
topics
prevfit <-stm(docs , vocab ,
K=20,
verbose=TRUE,
data=meta,
max.em.its=20)
topics <-labelTopics(prevfit , topics=c(1:20))
topics
library(stm)
#Process the data for analysis.
help("textProcessor")
temp<-textProcessor(documents=precorpus$Mission.statement.and.core.values, metadata = precorpus)
names(temp)
meta<-temp$meta
vocab<-temp$vocab
docs<-temp$documents
out <- prepDocuments(docs, vocab, meta)
docs<-out$documents
vocab<-out$vocab
meta <-out$meta
help("stm")
prevfit <-stm(docs , vocab ,
K=5,
verbose=TRUE,
data=meta,
max.em.its=5)
topics <-labelTopics(prevfit , topics=c(1:5))
topics
help("findThoughts")
findThoughts(prevfit, texts = precorpus$Full.text,  topics = 10,  n = 2)
help("findThoughts")
findThoughts(prevfit, texts = precorpus$Full.text,  topics = 5,  n = 2)
findThoughts(prevfit, texts = precorpus$Full.text,  topics = 2,  n = 2)
findThoughts(prevfit, texts = precorpus$Mission.statement.and.core.values,  topics = 2,  n = 2)
findThoughts(prevfit, texts = precorpus$Mission.statement.and.core.values,  topics = 3,  n = 2)
findThoughts(prevfit, texts = precorpus$Mission.statement.and.core.values,  topics = 5,  n = 2)
help("plot.STM")
plot.STM(prevfit, type="summary")
plot.STM(prevfit, type="labels", topics=c(15,4,5))
plot.STM(prevfit, type="perspectives", topics = c(19,10))
plot.STM(prevfit, type="summary")
plot.STM(prevfit, type="labels", topics=c(15,4,5))
plot.STM(prevfit, type="labels", topics=c(5,4,5))
plot.STM(prevfit, type="summary")
plot.STM(prevfit, type="labels", topics=c(5,4,5))
plot.STM(prevfit, type="perspectives", topics = c(5,5))
help(topicCorr)
mod.out.corr <- topicCorr(prevfit)  #Estimates a graph of topic correlations
plot.topicCorr(mod.out.corr)
topics   #shows topics with highest probability words
library(quanteda)
library(stm)
library(tm)
library(NLP)
library(openNLP)
setwd("C:/Users/sunil/Google Drive/Sem 3/Business analytics/Assignments/Assignment 5")
precorpus<- read.csv("Corpus1.csv",
header=TRUE, stringsAsFactors=FALSE)
names(precorpus)
head(precorpus)
require(quanteda)
help(corpus)
companycorpus<- corpus(precorpus$Mission.statement.and.core.values,
docnames=precorpus$Company.name,
docvar=NULL)
names(companycorpus)
summary(companycorpus)
head(companycorpus)
companycorpus<- toLower(companycorpus, keepAcronyms = FALSE)
cleancorpus <- tokenize(companycorpus,
removeNumbers=TRUE,
removePunct = TRUE,
removeSeparators=TRUE,
removeTwitter=FALSE,
verbose=TRUE)
str(cleancorpus)
#stop words from  https://github.com/arc12/Text-Mining-Weak-Signals/wiki/Standard-set-of-english-stopwords
swlist = c("a, about, above, across, after, again, against, all, almost, alone, along, already, also, although, always, am, among, an, and, another, any, anybody, anyone, anything, anywhere, are, area, areas, aren't, around, as, ask, asked, asking, asks, at, away, b, back, backed, backing, backs, be, became, because, become, becomes, been, before, began, behind, being, beings, below, best, better, between, big, both, but, by, c, came, can, cannot, can't, case, cases, certain, certainly, clear, clearly, come, could, couldn't, d, did, didn't, differ, different, differently, do, does, doesn't, doing, done, don't, down, downed, downing, downs, during, e, each, early, either, end, ended, ending, ends, enough, even, evenly, ever, every, everybody, everyone, everything, everywhere, f, face, faces, fact, facts, far, felt, few, find, finds, first, for, four, from, full, fully, further, furthered, furthering, furthers, g, gave, general, generally, get, gets, give, given, gives, go, going, good, goods, got, great, greater, greatest, group, grouped, grouping, groups, h, had, hadn't, has, hasn't, have, haven't, having, he, he'd, he'll, her, here, here's, hers, herself, he's, high, higher, highest, him, himself, his, how, however, how's, i, i'd, if, i'll, i'm, important, in, interest, interested, interesting, interests, into, is, isn't, it, its, it's, itself, i've, j, just, k, keep, keeps, kind, knew, know, known, knows, l, large, largely, last, later, latest, least, less, let, lets, let's, like, likely, long, longer, longest, m, made, make, making, man, many, may, me, member, members, men, might, more, most, mostly, mr, mrs, much, must, mustn't, my, myself, n, necessary, need, needed, needing, needs, never, new, newer, newest, next, no, nobody, non, noone, nor, not, nothing, now, nowhere, number, numbers, o, of, off, often, old, older, oldest, on, once, one, only, open, opened, opening, opens, or, order, ordered, ordering, orders, other, others, ought, our, ours, ourselves, out, over, own, p, part, parted, parting, parts, per, perhaps, place, places, point, pointed, pointing, points, possible, present, presented, presenting, presents, problem, problems, put, puts, q, quite, r, rather, really, right, room, rooms, s, said, same, saw, say, says, second, seconds, see, seem, seemed, seeming, seems, sees, several, shall, shan't, she, she'd, she'll, she's, should, shouldn't, show, showed, showing, shows, side, sides, since, small, smaller, smallest, so, some, somebody, someone, something, somewhere, state, states, still, such, sure, t, take, taken, than, that, that's, the, their, theirs, them, themselves, then, there, therefore, there's, these, they, they'd, they'll, they're, they've, thing, things, think, thinks, this, those, though, thought, thoughts, three, through, thus, to, today, together, too, took, toward, turn, turned, turning, turns, two, u, under, until, up, upon, us, use, used, uses, v, very, w, want, wanted, wanting, wants, was, wasn't, way, ways, we, we'd, well, we'll, wells, went, were, we're, weren't, we've, what, what's, when, when's, where, where's, whether, which, while, who, whole, whom, who's, whose, why, why's, will, with, within, without, won't, work, worked, working, works, would, wouldn't, x, y, year, years, yes, yet, you, you'd, you'll, young, younger, youngest, your, you're, yours, yourself, yourselves, you've, z") # stop words list
cleancorpus <- tokenize(companycorpus,
removeNumbers=TRUE,
removePunct = TRUE,
removeSeparators=TRUE,
removeTwitter=FALSE )
dfm.stem<- dfm(cleancorpus, toLower = TRUE,
ignoredFeatures = c(swlist, stopwords("english")),
verbose=TRUE,
stem=TRUE)
topfeatures.stem<-topfeatures(dfm.stem, n=50)
topfeatures.stem
#########################
### WORD CLOUD ########
#########################
library(wordcloud)
set.seed(142)   #keeps cloud' shape fixed
dark2 <- brewer.pal(7, "Set1")
freq<-topfeatures(dfm.stem, n=500)
wordcloud(names(freq),
freq, max.words=200,
scale=c(3, .1),
colors=brewer.pal(8, "Set1"))
#########################
### Find associations ###
#########################
dfm.tm<-convert(dfm.stem, to="tm")
names(dfm.tm)
findAssocs(dfm.tm,
c("data", "busi", "compani","custom","oracl","world","enterprise","make","big","solut","analyt"),
corlimit=0.7)
##########################
### Topic Modeling########
##########################
library(stm)
#Process the data for analysis.
help("textProcessor")
temp<-textProcessor(documents=precorpus$Mission.statement.and.core.values, metadata = precorpus)
names(temp)
meta<-temp$meta
vocab<-temp$vocab
docs<-temp$documents
out <- prepDocuments(docs, vocab, meta)
docs<-out$documents
vocab<-out$vocab
meta <-out$meta
help("stm")
prevfit <-stm(docs , vocab ,
K=5,
verbose=TRUE,
data=meta,
max.em.its=5)
topics <-labelTopics(prevfit , topics=c(1:5))
topics   #shows topics with highest probability words
help(topicCorr)
mod.out.corr <- topicCorr(prevfit)
plot.topicCorr(mod.out.corr)
prevfit <-stm(docs , vocab ,
K=5,
verbose=TRUE,
data=meta,
max.em.its=5)
topics <-labelTopics(prevfit , topics=c(1:5))
topics   #shows topics
findThoughts(prevfit, texts = precorpus$Mission.statement.and.core.values,  topics = 5,  n = 2)
plot.findThoughts(prevfit, texts = precorpus$Mission.statement.and.core.values,  topics = 5,  n = 2)
help(topicCorr)
mod.out.corr <- topicCorr(prevfit)
plot.topicCorr(mod.out.corr)
help(topicCorr)
mod.out.corr <- topicCorr(prevfit)
plot.topicCorr(mod.out.corr)
help(topicCorr)
mod.out.corr <- topicCorr(prevfit)
plot.topicCorr(mod.out.corr)
prevfit <-stm(docs , vocab ,
K=5,
verbose=TRUE,
data=meta,
max.em.its=5)
topics <-labelTopics(prevfit , topics=c(1:5))
topics   #shows topics with h
mod.out.corr <- topicCorr(prevfit)
plot.topicCorr(mod.out.corr)
library(quanteda)
library(stm)
library(tm)
library(NLP)
library(openNLP)
library(ggplot2)
library(ggdendro)
library(cluster)
library(fpc)
require(quanteda)
#using Corpus from quanteda - US President inaugural speeches
setwd("C:/Users/sunil/Google Drive/Sem 3/Business analytics/Assignments/Assignment 5")
speechdf<- read.csv("trumpVICTORYspeech.csv",
header=TRUE, stringsAsFactors=FALSE)
speechcorpus<- corpus(speechdf$Speech,
docnames=NULL,
docvar=NULL)
summary(speechcorpus)
speechcorpus<- toLower(speechcorpus, keepAcronyms = FALSE)
cleancorpus <- tokenize(speechcorpus,
removeNumbers=TRUE,
removePunct = TRUE,
removeSeparators=TRUE,
removeTwitter=FALSE,
verbose=TRUE)
#removing stopwords from the corpus
dfm<- dfm(cleancorpus,
toLower = TRUE,
ignoredFeatures =stopwords("english"),
verbose=TRUE,
stem=TRUE)
# Reviewing top features
topfeatures(dfm, 50)     # displays 50 features
negative<- read.csv("negativewordlist.csv")
positive<- read.csv("positivewords.csv")
negative<- read.csv("negativewordlist.csv", header = FALSE,stringsAsFactors=FALSE )
positive<- read.csv("positivewords.csv", header = FALSE,stringsAsFactors=FALSE )
names(negative)
names(positive)
wordlist<- read.csv("wordlist.csv", header = TRUE,stringsAsFactors=FALSE )
names(wordlist)
mydict <- dictionary(list(negative = wordlist$Negative.Words,
postive = wordlist$Positive.words))
dfm.sentiment <- dfm(cleancorpus, dictionary = mydict)
topfeatures(dfm.sentiment) # top features are aligned into the categories we define. its kind of a binomial dichotomy of the text
View(dfm.sentiment) #can be used to view anything
dfm
View(dfm.sentiment)
dfm.sentiment <- dfm(cleancorpus$text1, dictionary = mydict)
topfeatures(dfm.sentiment)
View(dfm.sentiment)
dfm.sentiment <- dfm(cleancorpus$text2, dictionary = mydict)
topfeatures(dfm.sentiment)
dfm.sentiment <- dfm(cleancorpus$text3, dictionary = mydict)
topfeatures(dfm.sentiment)
wordlist<- read.csv("wordlist.csv", header = TRUE,stringsAsFactors=FALSE )
names(wordlist)
mydict <- dictionary(list(negative = wordlist$Negative.Words,
postive = wordlist$Positive.words))
dfm.sentiment <- dfm(cleancorpus, dictionary = mydict)
topfeatures(dfm.sentiment)
View(dfm.sentiment)
dfm.sentiment <- dfm(cleancorpus$text3, dictionary = mydict)
topfeatures(dfm.sentiment)
library(dplyr)
require(magrittr)
library(tm)
library(ggplot2)
library(stringr)
library(NLP)
library(openNLP)
#passing Full Text to variable news_2015
news_2015<-speechdf$Speech
#the same script can be run to analyse. pass the required text to news_2015. the script can be used as a package
#Cleaning corpus
stop_words <- stopwords("SMART")
## additional junk words showing up in the data
stop_words <- c(stop_words, "said", "the", "also", "say", "just", "like","for",
"us", "can", "may", "now", "year", "according", "mr")
stop_words <- tolower(stop_words)
news_2015 <- gsub("'", "", news_2015) # remove apostrophes
news_2015 <- gsub("[[:punct:]]", " ", news_2015)  # replace punctuation with space
news_2015 <- gsub("[[:cntrl:]]", " ", news_2015)  # replace control characters with space
news_2015 <- gsub("^[[:space:]]+", "", news_2015) # remove whitespace at beginning of documents
news_2015 <- gsub("[[:space:]]+$", "", news_2015) # remove whitespace at end of documents
news_2015 <- gsub("[^a-zA-Z -]", " ", news_2015) # allows only letters
news_2015 <- tolower(news_2015)  # force to lowercase
## get rid of blank docs
news_2015 <- news_2015[news_2015 != ""]
# tokenize on space and output as a list:
doc.list <- strsplit(news_2015, "[[:space:]]+")
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
# remove terms that are stop words or occur fewer than 5 times:
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
term.table <- term.table[names(term.table) != ""]
vocab <- names(term.table)
# now put the documents into the format required by the lda package:
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
#############
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents (1)
W <- length(vocab)  # number of terms in the vocab (1741)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (56196)
term.frequency <- as.integer(term.table)
# MCMC and model tuning parameters:
K <- 10
G <- 3000
alpha <- 0.02
eta <- 0.02
# Fit the model:
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
t2 <- Sys.time()
## display runtime
t2 - t1
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
news_for_LDA <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
library(LDAvis)
library(servr)
# create the JSON object to feed the visualization:
json <- createJSON(phi = news_for_LDA$phi,
theta = news_for_LDA$theta,
doc.length = news_for_LDA$doc.length,
vocab = news_for_LDA$vocab,
term.frequency = news_for_LDA$term.frequency)
serVis(json, out.dir = 'vis', open.browser = TRUE)
